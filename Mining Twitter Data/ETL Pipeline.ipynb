{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38332bit95cab9994d5d4ce8b2760e46dd966f88",
   "display_name": "Python 3.8.3 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ETL Pipeline is a set of processes extracting data from an input source, transforming the data, and loading it into an output destination such as a database, data mart, or a data warehouse for reporting, analysis, and data synchronization. The letters stand for Extract, Transform, and Load.\n",
    "\n",
    "For this Pipeline, the functions below will do the following:\n",
    "\n",
    "1. Function to connect to twitter and scrapes \"Eskom_SA\" tweets.\n",
    "<br>\n",
    "<br>\n",
    "2. Cleans/Processes the tweets from the scraped tweets which will create a dataframe with two new columns using the following functions: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a) Hashtag Remover from Analyse Functions\n",
    "<br>\n",
    "<br>\n",
    "3. Functions which connects to your SQL database and uploads the tweets into the table you store the tweets in the database.\n",
    "\n",
    "authors: Nthabeleng Vilakazi, Nelisiwe Mabanga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy           # To consume Twitter's API\n",
    "import pandas as pd     # To handle data\n",
    "import numpy as np      # For numerical computation\n",
    "import json\n",
    "# For plotting and visualization:\n",
    "from IPython.display import display\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer and Access details\n",
    "\n",
    "Fill in your Consumer and Access details you should have recieved when applying for a Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer:\n",
    "CONSUMER_KEY    = 'xxxxxxx' \n",
    "CONSUMER_SECRET = 'xxxxxx'\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = 'xxxxxx'\n",
    "ACCESS_SECRET = 'xxxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API's setup:\n",
    "def twitter_setup():\n",
    "    \"\"\"\n",
    "    Utility function to setup the Twitter's API\n",
    "    with access and consumer keys from Twitter.\n",
    "    \"\"\"\n",
    "\n",
    "    # Authentication and access using keys:\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    # Return API with authentication:\n",
    "    api = tweepy.API(auth, timeout=1000)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1:\n",
    "\n",
    "This function scrapes _\"Eskom_SA\"_ tweets from Twitter. \n",
    "\n",
    "Function Specifications:\n",
    "- The function returns a dataframe with the scraped tweets with just the \"_Tweets_\" and \"_Date_\". \n",
    "- Will take in the ```consumer key,  consumer secret code, access token``` and ```access secret code```.\n",
    "\n",
    "NOTE:\n",
    "The dataframe should have the same column names as those in your SQL Database table where you store the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_df(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_SECRET ):\n",
    "    extractor = twitter_setup()\n",
    "    tweets = extractor.user_timeline(screen_name=\"eskom_sa\", count=200, include_rts=False)\n",
    "    data = pd.DataFrame(data=np.column_stack([[tweet.text for tweet in tweets],\n",
    "                                            [(tweet.created_at) for tweet in tweets]]),\n",
    "                                            columns=['Tweets','Date'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2: Removing hashtags and the municipalities\n",
    "\n",
    "Write a function which extracts the hashtags and municipalities into it's own column in a new dataframe\n",
    "\n",
    "Function Specifications:\n",
    "- The function should take in the pandas dataframe created in Function 1 and return a new pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def extract_municipality_hashtags(df):\n",
    "    # your code here\n",
    "    mun_dict = {\n",
    "                '@CityofCTAlerts' : 'Cape Town',\n",
    "                '@CityPowerJhb' : 'Johannesburg',\n",
    "                '@eThekwiniM' : 'eThekwini' ,\n",
    "                '@EMMInfo' : 'Ekurhuleni',\n",
    "                '@centlecutility' : 'Mangaung',\n",
    "                '@NMBmunicipality' : 'Nelson Mandela Bay',\n",
    "                '@CityTshwane' : 'Tshwane'}\n",
    "    cities = []\n",
    "    mun = 0\n",
    "    count = 1\n",
    "    Tweets = df['Tweets']\n",
    "    for x in Tweets:\n",
    "        for keys in mun_dict.keys():\n",
    "            if keys in x:\n",
    "                cities.append(mun_dict[keys])\n",
    "                mun += 1\n",
    "        if mun == count:\n",
    "            count += 1\n",
    "        else:\n",
    "            cities.append(np.nan)    \n",
    "    df['municipality'] = cities\n",
    "    \n",
    "    hash_tags = []\n",
    "    for x in Tweets:\n",
    "        strsplit = []\n",
    "        innerlist = []\n",
    "        mun = 0\n",
    "        strsplit = x.split()\n",
    "        for var in strsplit:\n",
    "            if var[0] == '#':\n",
    "                innerlist.append(var.lower())\n",
    "                mun += 1\n",
    "        if mun != 0:\n",
    "            hash_tags.append(innerlist)\n",
    "        else:\n",
    "            hash_tags.append(np.nan)\n",
    "    df['hashtags'] = hash_tags\n",
    "    return df\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3: Updating SQL Database with pyODBC\n",
    "\n",
    "- This function connects and updates your SQL database. \n",
    "\n",
    "Function Specifications:\n",
    "- The function should take in a pandas dataframe created in Function 2. \n",
    "- Connect to your SQL database.\n",
    "- Update the table you store your tweets in.\n",
    "- Not return any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyodbc_twitter(connection, df, twitter_table):\n",
    "    \n",
    "    cur.execute(\n",
    "    \"\"\"\n",
    "    DROP TABLE IF EXISTS twitter_table;\n",
    "    CREATE TABLE twitter_table ([Tweets] VARCHAR(300), [Date] VARCHAR(50), [municipality] VARCHAR(20), [hashtags] VARCHAR(100)); \n",
    "     \"\"\"\n",
    "        \n",
    "    )\n",
    "    \n",
    "#     df2 =extract_municipality_hashtags(twitter_df(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_SECRET ))\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        cur.execute(\"\"\"INSERT INTO dbo.twitter_table([Tweets],[Date],[municipality], [hashtags]) values(?,?,?,?)\"\"\",\n",
    "        str(row['Tweets']), str(row['Date']), str(row['municipality']), str(row['hashtags'])\n",
    "        )\n",
    "        \n",
    "    cur.close()\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}